---
title : ECON 6931 - R Programming (Code Supplement)
output:
  html_document:
    toc: true
    theme: spacelab
---

```{r setOpts, echo = FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, warning = FALSE, message = FALSE)
```

<h1>Week 3 Programming Supplement</h1>


<h2>Logical Operators</h2>

Often in your code, you'll want to do/not do something or select / not select some data based on a logical condition (a statement that evaluates to TRUE or FALSE). Here are some examples of how to construct these statements in R.

```{r logicalCond1}
# "and" logic is expressed with "&"
TRUE & TRUE   # TRUE
TRUE & FALSE  # FALSE
FALSE & FALSE # FALSE
-5 < 5 & 3 > 2 # TRUE

# "or" logic is expressed with "|"
TRUE | TRUE    # TRUE
TRUE | FALSE   # TRUE
FALSE | FALSE  # FALSE
3 < 8 | 8 > 19 # TRUE
```

The most common operators used to generate logicals are `>`, `<`, `==`, and `!=`

```{r logicalCond2}
# "equality" logic is specified with "=="
3 == 3   # TRUE
4 == 4.1 # FALSE

# "not" logic is specified with !. In a special case, != signifies "not equal"
!TRUE            # FALSE
!FALSE           # TRUE
! (TRUE | FALSE) # FALSE
4 != 5           # TRUE

# "greater than" and "less than" logic are specified in the way you might expect
5 < 5  # FALSE
6 <= 6 # TRUE
4 > 2  # TRUE
3 >= 3 # TRUE
```

As we learned in week two, you can use vectors of logicals (TRUE and FALSE) to subset other objects. As a general rule, when you put a vector on the left-hand side of a logical condition like `==` or `>`, you will get back a vector as a result.

```{r logicalCond3}
# Load some data
data("mtcars")

# Create a logical index. Note that we get a VECTOR of logicals back
bigCarIndex <- mtcars$wt > 4

# Taking the SUM of a logical vector tells you the number of TRUEs.
# Taking the MEAN of a logical vector tells you the proportion of TRUEs
sum(bigCarIndex)
mean(bigCarIndex)

# You can use this for subsetting
mtcars[bigCarIndex,]
```

<h2>Factors</h2>

Imagine that you want to build a model of the relationship between resource wealth and quality-of-life outcomes like life expectancy. You got out to the World Bank to grab some data, and the dataset you get includes a column called "region" with values like "Africa", "European Union", and "South America". How can you use this variable in a model or for generating region-by-region summary stats? This is where R's **factor type** comes in.

```{r factorVar1}
# Sample Data
wbDF <- data.frame(
    country = c("Egypt", "Cyprus", "Nicaragua", "Colombia", "Germany")
    , region  = c("Africa", "European Union", "South America", "South America", "European Union")
    , lifeexp = c(74.5, 78.0, 75.6, 72.3, 81.9)
)

# Check classes...see that "region" is a "Factor" by default!
str(wbDF)
```

What does it mean for `region` to be a factor? Essentially, a factor is a categorical variable. R uses a cool trick to save memory when storing factors...internally, R will convert factor values to integers and then keep aroudn a single table the tells it, e.g., that 1 = "Africa", 2 = "European Union", etc..

```{r factorVar2}
# Check it out! R has assigned integer values to the "region" variable
as.integer(wbDF$region)

# But you can also access the character values if you want
as.character(wbDF$region)

# For more, see:
str(wbDF$region)
levels(wbDF$regions)
```

<h2>File Paths</h2>

Whenever you find yourself reading data into R or writing data out of it, you will need to work with file paths. File paths are just addresses on your computer's file system. These paths can either be *relative* (expressed as steps above/below your current location) or *absolute* (full addresses). 

All relative paths in R are relative to your **working directory**, a single location that you can set and reset any time in your session.

```{r setwdExample, eval = FALSE, echo = TRUE}
# Check and then change the current working directory
getwd()
setwd("~/repos/sandbox")

# Reference a file with a full path
myDF <- read.csv(file = "~/repos/sandbox/data/some_data.csv")

# Reference a file with a relative path
myDF <- read.csv(file = "./data/some_data.csv")
```

R provides a few other utilities for working with file paths and directory structures from inside your code. Check it out!

```{r moarFilez, eval = FALSE, echo = TRUE}
# List all the files in some directory and put the list in a vector
theFiles <- list.files(path = "~/repos/some_folder/docs/")

# Create a directory if it doesn't exist
if (!dir.exists("~/repos/some_folder/docs/slides")) {
    dir.create("~/repos/some_folder/docs/slides")
}

# Check if a file exists
myFileExists <- file.exists("~/repos/some_folder/docs/report.xlsx")
```

<h2>R + Excel</h2>

In the Economics / Business world (and many other areas!), Microsoft Excel is pretty much unavoidable. You'll get data from the internet, your colleagues, clients, etc. in Excel format and may want to work with it in R. There are a few packages for doing this, but in this course we'll focus on [openxlsx](https://cran.r-project.org/web/packages/openxlsx/openxlsx.pdf).

NOTE: This package requires certain Java components that you may not have on your machine. If you run into issues, I recommend 1) installing an updated version of [JRE](http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html) or 2) exploring other packages like [xlsx](https://cran.r-project.org/web/packages/xlsx/xlsx.pdf) or [readxl](https://cran.r-project.org/web/packages/readxl/readxl.pdf).

```{r readingExcel, eval = FALSE, echo = TRUE}
# Read an Excel file into a data.frame
library(openxlsx)
newDF <- openxlsx::read.xlsx(xlsxFile = "~/repos/sandbox/data/stockData.xlsx")
```

You can also use this package to write Excel files. You can do really complicated stuff (like conditional formatting, named ranges, and live formulas) from inside of R. It's tough to set up at first, but can be VERY useful if you find yourself spending a lot of time running routine reports whose format is the same from update to update.

```{r writingExcel, eval = FALSE, echo = TRUE}
# load mtcars
data("mtcars")

# create a workbook object in R
testWB <- openxlsx::createWorkbook()

# Add sheets and data
openxlsx::addWorksheet(testWB, sheetName = "car_data")
openxlsx::writeData(testWB, sheet = "car_data", x = mtcars)

# Write out the file
openxlsx::saveWorkbook(wb = testWB, file = "~/sandbox/data/testing.xlsx")
```

<h2>Text Processing Exercise</h2>

<h3>Common Preprocessing Steps</h3>

In this section, we're going to revisit the [Shakespeare corpus](https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt) we looked at in Week 2 and implement a basic "keyword extraction" pipeline. Let's start by loading it and doing some common string preprocessing on it.

```{r startTextAnalysis}
# Grab a random 5000 lines from the corpus (skipping a bunch of that MIT text at the beginning)
shakespeareFile <- "https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt"
bsText <- readLines(con = shakespeareFile, n = 10000)[5001:10000]

# 1 - Convert everything to lowercase
bsText <- tolower(bsText)

# 2 - Trim leading and trailing whitespace (e.g. change "   a" to "a")
bsText <- trimws(bsText)

# 3 - Remove empty lines
bsText <- bsText[sapply(bsText, nchar)>0]
```

<h3>Intro to Regular Expressions</h3>

After applying these basic steps, we're going to want to do some more powerful things like removing or replacing text based on particular character patterns. It is time to enter the mystical world of [regular expressions](https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html).

Regular expressions (used in many programming languages) offer a way to express complex pattern matching. Let's work through some examples to demonstrate the properties.

```{r regex1}
# 4 - Remove punctuation
bsText <- gsub(pattern = ";|,|!|?|\\.|\\:|<|>|\\]|\\[", replacement = "", bsText)
```

<h3>Intro to Regular Expressions</h3>

You can run `sample(bsText, 10)` and see that our data are looking cleaner...but we still have work to do! Next, let's use regular expressions (commonly just called "regex") to handle some other issues.

```{r moreRegex}
# 5 - split some common contractions into two words
bsText <- gsub("he('s)", "he is", bsText)
bsText <- gsub("'ll", " will", bsText)

# 6 - Change any numbers to __number__
bsText <- gsub("[:digit:]", "__number__", bsText)
```

<h3>Tokenization</h3>

The next task we need to acomplish is **tokenizing** our text, i.e. splitting lines and sentences into individual words. These individual words can then be used downstream to get build a language model and identify key terms.

```{r tokenization}
# 7- Loop over the vector of lines, split on whitespace, create a list of data.frames
library(stringr)
library(data.table)
allWords <- lapply(
    X = bsText
    , FUN = function(thisLine){
        return(data.frame(words = str_split(thisLine, " ")[[1]]))
    }
)

# 8 - Put into a data.table and print that just to make sure it worked
wordDT <- data.table::rbindlist(allWords)
print(wordDT[1:10], row.names = FALSE)
```

<h3>Counting Words in Text</h3>
    
Now that our data are a bit cleaner, it's time to try finding key terms! Broadly speaking, "key terms" in a body of text are those that more common in the text than they are in the language as a whole (e.g. "the" will never be a key word). We aren't looking at any actual data on the distribution of words in Shakespeare-era English in this exercise, so we'll just drop the top 20 words and call the next 20 "key".

The `data.table` package makes this operation easy to carry out.

```{r countTheWords}
# 9 - Get Word counts and sort by those counts
wordCountDT <- wordDT[, .N, by = words]
data.table::setnames(wordCountDT, old = "N", new = "word_count")
data.table::setorder(wordCountDT, -word_count)
print(wordCountDT[1:10], row.names = FALSE)

key_words <- wordCountDT[21:40]
```

<h2>Specialness of NAs</h2>

`NA` is a special object in R, used to capture the idea of "a value whose value is unknown". Confusing, right? We're going to go through a few examples to get you feeling comfortable with missing values. They're an inevitability in real-world data.

**PRO TIP**: See `?NA` for R's documentation on the nuances of `NA`

```{r introToNAs}
# Create a vector w/ missing data
some_nums <- c(1,2,NA, 6, NA, 8)
print(some_nums)

# Use is.na() to get a vector of TRUE/FALSE for the question "is this element NA?"
is.na(some_nums)

# Confirm that even w/ NAs, R still knows this is a numeric vector
class(some_nums)
```

<h2>Strategy 1: Total Eradication</h2>

The first approach you may take to dealing with `NA` values is to simply drop them from your data. If you don't think these missing data have any business value and your dataset is big enough that you can afford to drop some rows / columns, this is the right move for you.

```{r removeNAs}
# Removing NAs for vectors
top5 <- c("Wale", "Chance", NA, "Lupe Fiasco", "Shad", "Kanye", NA)
print(top5)
top5cleaned <- top5[!is.na(top5)]
print(top5cleaned)

# Removing rows with ANY NAs for data.frames
myDF <- data.frame(
    x = c(1, 2, NA, 4)
    , y = c(NA, TRUE, FALSE, TRUE)
    , z = c("hey", "there", NA, "friends")
)
cleanDF <- myDF[complete.cases(myDF), ]
```

<h2>Strategy 2: Handle on Subsets</h2>

You may find the "remove all the NAs everywhere" strategy a bit too aggreesive for your use case. If you have a 100-variable dataset and a single variable (column) is 90\% NA values, do you really want to drop every row where that variable is NA? A better approach might be to selectively subset out columns where missing values are most severe before using `complete.cases` to remove rows.

```{r subsetNAwisely}
# Create a dataframe where some variable have more NAs than others
testDF <- data.frame(
    var1 = sample(c(rnorm(99), NA), 200, replace = TRUE)
    , var2 = sample(c(rnorm(50), rep(NA, 50)), 200, replace = TRUE)
    , var3 = sample(c(rnorm(5), rep(NA, 95)), 200, replace = TRUE)
)

# Find columns that are more than 90% missing values
.percent_na <- function(a_vector){
    return(sum(is.na(a_vector)/length(a_vector)))
}
colsToDrop <- apply(testDF, MARGIN = 2, .percent_na) > 0.9
cleanDF <- testDF[, !colsToDrop]

# Remove rows w/ remaining NAs
cleanDF <- cleanDF[complete.cases(cleanDF),]
```

<h2>Strategy 3: Imputation</h2>

A final strategy, particularly useful in modeling contexts, is to use some [imputation strategy](https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/) to replace `NA` values with reasonable alternatives. One common approach (and my favorite), the `roughfix` method. It works like this:
- For numeric columns, replace NAs with the column median
- For categorical columns, replace NAs with the most common value

```{r imputation}
# Create a dataframe where some variable have more NAs than others
testDF <- data.frame(var1 = sample(c(rnorm(99), NA), 500, replace = TRUE),
                     var2 = sample(c(rnorm(70), rep(NA, 30)), 500, replace = TRUE),
                     var3 = sample(c(rnorm(85), rep(NA, 15)), 500, replace = TRUE))

# Clean up w/ roughfix
library(randomForest)
cleanDF <- randomForest::na.roughfix(testDF)
```

<h2>Intro to the Base Plotting System</h2>

R is famous, in part, for its ability to create production-quality plots within the default graphics package it ships with. This plotting paradigm is often referred to as "the base plotting system", and we're going to walk through a few examples of it this week.

The essential idea of the base plotting system is to build up plots in layers. You first create a simple 1-variable line plot, for example, then "add on" a legend, more variables, other plot types, etc. We'll try a few examples using the sample data created below.

```{r getPlotData}
# Load up the famous iris dataset
data("iris")
head(iris, n = 10)
```

Let's start with a simple line plot to answer the question *are sepal length and sepal width related?*

```{r baseLinePlot}
# Create a simple line plot
plot(x = iris$Sepal.Length, y = iris$Sepal.Width, type = "p")

# Try again, but with labels!
plot(x = iris$Sepal.Length, y = iris$Sepal.Width, main = "My Second R plot!", 
     xlab = "sepal length", ylab = "sepal width", type = "p")

# Try it AGAIN, this time coloring by species and a legend
plot(x = iris$Sepal.Length, y = iris$Sepal.Width, main = "My Third R plot!", 
     xlab = "sepal length", ylab = "sepal width", type = "p", col = iris$Species,
     bg = iris$Species, pch = 21)
legend(x = 7,y = 4.3,unique(iris$Species), col = 1:length(iris$Species), pch = 1)
```

The base plotting system can be a great tool for quick exploratory analysis of data, such as examination of the distribution of variables in your data.

```{r histAndDensity}
# Minimal Histogram
hist(iris$Petal.Length)

# Better histogram
hist(iris$Petal.Length, main = "Distribution of petal length",
     xlab = "petal length", breaks = 25)

# Empirical density
plot(density(iris$Petal.Length), main = "Empirical density of petal length", col = "blue")
```

You can add more than one variable to these plots! Let's compare the densities of Sepal length by species

```{r compareDensities}

# Overlay densities of Petal length by species
plot(density(iris[iris$Species == "setosa", "Petal.Length"]), 
     main = "Empirical density of petal length", col = "blue",
     xlim = c(0, 7), ylim = c(0, 2.5))
lines(density(iris[iris$Species == "versicolor", "Petal.Length"]), col = "red")
lines(density(iris[iris$Species == "virginica", "Petal.Length"]), col = "black")

# Add a legend
legend(x = 5.5,y = 2.25, unique(iris$Species), col = c("blue", "red", "black"), pch = 1)
```

You can control the plotting options to make a grid of plots. The code below creates a 2x2 grid with a density for Sepal Width and scatter plots of the other three variables against sepal width.

```{r gridOfPlots}
# Set global options
par(mfrow = c(2,2))

# Dump in some plots
plot(density(iris$Sepal.Width), main = "Empirical density of petal length", col = "red")
plot(x = iris$Sepal.Width, y = iris$Sepal.Length, col = iris$Species, bg = iris$Species, pch = 21)
plot(x = iris$Sepal.Width, y = iris$Petal.Length, col = iris$Species, bg = iris$Species, pch = 21)
plot(x = iris$Sepal.Width, y = iris$Petal.Width, col = iris$Species, bg = iris$Species, pch = 21)

# reset options
par(mfrow = c(1,1))
```

<h2>A Note On Graphics Devices</h2>

When R (or any other program!) creates plots, it needs to know where to put them! When you call `plot()` or other commands from within and RStudio session, the default is to just display the resulting figure in the "Plots" pane. However, you can use other **graphics devices** (places to put visual output) to tell R to put your figures elsewhere.

```{r path2png, eval = FALSE, echo = TRUE}
# Create 10 plots in a loop
outDir <- getwd()
for (i in 1:10){
    # Open a connection to a .png file
    filePath <- paste0(outDir, "/plot_", i, ".png")
    png(filePath)
    
    # Write out a plot to that file
    plot(x = rnorm(100), y = rnorm(100))
    
    # Close the connection to that file
    dev.off()
}
```
