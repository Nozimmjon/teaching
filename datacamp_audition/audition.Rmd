---
title: "Building Processing Pipelines in data.table"
output: 
    html_document:
        code_folding: show
        toc: true
        toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
API_KEY <- "qWGKLxwNk7zMhybtqsp5"
Quandl::Quandl.api_key(API_KEY)
```

# Introduction

In this tutorial, we will walk through how to build efficient, reusable processing pipelines in R using the `data.table` library. This tutorial assumes that you are comfortable with functional programming in R and the fundamentals of the `data.table` package. If you feel overwhelmed at any point and need a refresher, those courses can be found here:

- [Writing Functions in R](https://www.datacamp.com/courses/writing-functions-in-r)
- [Data Analysis in R, the data.table way](https://www.datacamp.com/courses/data-table-data-manipulation-r-tutorial)

# Getting Started

Consider the following problem: you are an analyst at a large consumer goods company, researching the relationship between overall inflation in the United States and the changes in prices of particular key commodities. After some Googling, you discover that most of the data you need is available from public sources like [Quandl](https://www.quandl.com/) and the [St. Louis Federal Reserve](https://fred.stlouisfed.org/). Awesome!

**Inflation Data**

```{r}
# inflation data
cpiDF <- quantmod::getSymbols(
    Symbols = "CPIAUCSL"
    , src = "FRED"
    , auto.assign = FALSE
)
```

```{r}
head(cpiDF)
```

```{r}
tail(cpiDF)
```

**Spot price of copper**

```{r}
# inflation data
copperDF <- Quandl::Quandl(
    code = "LME/PR_CU"
    , start_date = "2001-01-01"
    , end_date = "2018-03-01"
)
```

```{r}
head(copperDF)
```

```{r}
tail(copperDF)
```

## Practical Problem Solving

This is promising, but there are some data issues you'll need to solve here! If you want to analyze the relationship between CPI inflation and metals prices, you'll need to do some data wrangling. Namely:

- The metals data from Quandl has many columns that all mean "price"
- Inflation data is available monthly but we metals spot prices are reported daily
- Need to join together inflation data and metals prices into one data frame
- Need to turn raw prices into percentage changes to fairly compare the relationship
- You may want to add additional metals to the dataset

In what follows, I'll show how `data.table` is well-suited for these tasks. It provides a set of powerful, efficient tools for the complicated data manipulation we need to do.

Let's get started!

```{r}
suppressPackageStartupMessages({
    library(Quandl)
    library(quantmod)
    library(corrplot)
    library(lubridate)
    library(data.table)
})
```

### 1. Standardizing Outputs

To make our metals data comparable to other datasets like inflation, we need to standardize the representation. Let's use `data.table` to establish the contract that every dataset has two columns: `obs_date` and one column representing the core concept.

```{r}
copperDF <- Quandl::Quandl(
    code = "LME/PR_CU"
    , start_date = "2001-01-01"
    , end_date = "2018-03-01"
)

# Get a data.table version of the data and subset out what we don't care about
copperDT <- data.table::as.data.table(copperDF)[, .(Date, `Cash Buyer`)]

# Note that we can subset easily
data.table::setnames(copperDT, c("obs_date", "copper_spot"))

# check it out
copperDT
```

### 2. Merging data.tables

This is looking good! But what if we wanted to get another metals dataset? We can apply the same processing, then use the `merge` method provided by `data.table`. This is similar to the `merge` function you may have seen in base R, but it will take advantages of some special characteristics of `data.table`s to make it super fast.

```{r}
nickelDF <- Quandl::Quandl(
    code = "LME/PR_NI"
    , start_date = "2001-01-01"
    , end_date = "2018-03-01"
)

# Get a data.table version of the data and subset out what we don't care about
nickelDT <- data.table::as.data.table(nickelDF)[, .(Date, `Cash Buyer`)]

# Note that we can subset easily
data.table::setnames(nickelDT, c("obs_date", "nickel_spot"))

# check it out
nickelDT
```

Nickel looks good! Let's merge them:

```{r}
# set keys on the data.tables to make merging fast
data.table::setkeyv(copperDT, "obs_date")
data.table::setkeyv(nickelDT, "obs_date")

# Merge them
metalsDT <- merge(
    x = copperDT
    , y = nickelDT
    , by = "obs_date"
    , all = TRUE
)

# check it out
metalsDT
```

## 3. Scaling to many tables

The approach above works fine if we have a small handful of commodities, but would be painful if we wanted to add many more of frequently change the list. Manually changing the names of those tables would get annoying pretty fast...there has to be a better way!

These situations are a great use case for one of R's most powerful funcitonal programming tools, `Reduce`. To demonstrate the power of this, I'm going to create two additional tables (with fake data!) and merge them into our copper-and-nickel dataset.

```{r}
# Create fake data
fakeDT1 <- data.table::copy(copperDT)
data.table::setnames(fakeDT1, c("obs_date", "kryptonite_spot"))

fakeDT2 <- data.table::copy(copperDT)
data.table::setnames(fakeDT2, c("obs_date", "stainless_steel_spot"))

# Create a list of tables
metals_data <- list(
    copperDT
    , nickelDT
    , fakeDT1
    , fakeDT2
)

# Merge everything together
bigDT <- Reduce(
    f = function(x, y){merge(x, y, all = TRUE, by = "obs_date")}
    , x = metals_data
)

# check it out
bigDT
```

```{r, echo = F}
# clean up all that stuff
rm(list = c("bigDT", "metals_data", "copperDT", "nickelDT", "fakeDT1", "fakeDT2"))
```

As you can see above, `Reduce` effectively merged all the tables together without leaving behind lots of intermediate artifacts or requiring us to manually specify too much. However, we still had to manually populate `metals_data`...let's change that.

When designing data pipelines, it's usually valuable to separate *configuration* (code that says what to do) from *execution* (code that says how to do it). Setting up a *configuration* involves answering the question "what information is changing from dataset to dataset?". In the case of our metals prices, two things are changing:

- the code used by Quandl to identify the data
- the name we give to the price series in R

Given this, we can create a configuration like the following:

```{r}
spot_codes <- c(
    "copper_spot" = "LME/PR_CU"
    , "nickel_spot" = "LME/PR_NI"
)

print(spot_codes)
```

Finally, to complete our pipeline for pulling metals data, we need to implement a function that can take in this configuration and execute the steps we went through carefully above.

```{r}
# Define the function
get_metal_prices <- function(qcodes){
    
    responseList <- lapply(
        X = names(qcodes)
        , qcodes = qcodes
        , function(metal_name, qcodes){
            
            print(sprintf("Fetching data for %s...", qcodes[[metal_name]]))
            
            # Grab the dataset and immediately drop columns we don't care about
            responseDT <- data.table::as.data.table(
                Quandl::Quandl(
                    code = qcodes[[metal_name]]
                    , start_date = "2001-12-31"
                    , end_date = "2018-03-12"
                )
            )[, .SD, .SDcols = c("Date", "Cash Buyer")]
            
            # Change names
            data.table::setnames(responseDT, c("obs_date", metal_name))
        }
    )
    
    # merge datasets on time
    outDT <- Reduce(
        f = function(x, y){merge(x, y, all = TRUE, by = "obs_date")}
        , x = responseList
    )
    return(outDT)
}

# Test it out with our configuration!
metalsDT <- get_metal_prices(spot_codes)
metalsDT
```

Now that we know our function is working, we can add a few more datasets with minimal new code! Let's add cobalt, aluminum, and tin.

```{r}
spot_codes <- c(
    "cobalt_spot" = "LME/PR_MO"
    , "aluminum_spot" = "LME/PR_AL"
    , "tin_spot" = "LME/PR_TN"
    , "copper_spot" = "LME/PR_CU"
    , "nickel_spot" = "LME/PR_NI"
)

metalsDT <- get_metal_prices(spot_codes)
metalsDT
```

## 4. Getting macro data

Now that we're pros at building data pipelines with `data.table`, let's apply the same approach to our inflation data! For the purpose of this case study we'll only concern ourselves with CPI and PCE inflation, but will follow the same principles of separating *configuration* and *execution* in case we want to add more macro data later.

Similar to the metals case, the configuration portion here has two fields:

- the code used by Quandl to identify the data
- the name we give to the macro series in R

```{r}
macro_codes <- c(
    "cpi_inflation" = "CPIAUCSL"
    , "pce_inflation" = "PCEPILFE"
)

print(macro_codes)
```

This is great! Having a similarly-structured configuration is a good sign that we'll be able to reuse some execution code. Check it out:

```{r}
# Define a function to pull macro data
get_macro_data <- function(mcodes){
    responseList <- lapply(
        X = names(mcodes)
        , mcodes = mcodes
        , FUN = function(macro_name, mcodes){
            
            # get data and convert to data.table
            macroDT <- data.table::as.data.table(
                quantmod::getSymbols(
                    Symbols = mcodes[[macro_name]]
                    , src = "FRED"
                    , auto.assign = FALSE
                )
            )
            
            # change names in place
            data.table::setnames(macroDT, c("obs_date", macro_name))
        }
    )
    
    # merge datasets on time
    outDT <- Reduce(
        f = function(x, y){merge(x, y, all = TRUE, by = "obs_date")}
        , x = responseList
    )
    return(outDT)
}
```

Looks pretty familiar, right? Let's confirm that it works for inflation data.

```{r}
macro_codes <- c(
    "cpi_inflation" = "CPIAUCSL"
    , "pce_inflation" = "PCEPILFE"
)
macroDT <- get_macro_data(macro_codes)
macroDT
```

## 5. Combining datasets from different places

Now that we have a pipeline for pulling metals data and macro data, it's time to combine them! Note that both datasets have an `obs_date` column, so we should just be able to join them, right?

Maybe.

The metals data are daily prices while the inflation data are monthly averages. If we join them on the `obs_date` column, we'll be joining, for example, average inflation over January 2017 to the price of copper on January 1st 2017. Metals prices can move a lot over the course of one month, so this is probably not a good approach.

Instead, we'll use `data.table` operations to generate a table of monthly average metals prices, and join *those* to the macro data.

```{r}
# define a helper function that takese in a date and produces the corresponding
# date for the 1st day of that date's year and month
.ym <- function(some_date){
    lubridate::make_date(
        year = lubridate::year(some_date)
        , month = lubridate::month(some_date)
        , day = 1L
    )
}

# Use the := operator to add this transformed date to our table
metalsDT[, obs_month := .ym(obs_date)]

# View the first month of data
metalsDT[1:30, .(obs_date, obs_month)]
```

That wasn't *too* bad, right? The code above took advantage of a couple powerful `data.table` features:

- `:=` is special syntax to add new columns.
- calling `some_function(column_name)` applies that function over the column
- inside the `[]` of a `data.table`, we can use `.(name1, name2)` to subset on columns

To aggregate by monthly, we'll apply the `mean` function over multiple columns using another special tool in `data.table` called `.SD`. `.SD` is a special selector intended for exactly this purpose.

```{r}
monthlyMetalsDT <- metalsDT[, lapply(.SD, function(x){mean(x, na.rm = TRUE)})
                           , .SDcols = setdiff(names(metalsDT), c("obs_date", "obs_month"))
                           , by = "obs_month"]

monthlyMetalsDT[1:10]
```

We're ALMOST ready to start working with this dataset! The code below merges the macro data and monthly metlas prices. Notice that I've used `all.x = FALSE` to say "only keep rows where we had both metals data and macro data".

```{r}
# Join them together!
fullDT <- merge(
    x = monthlyMetalsDT
    , y = macroDT
    , by.x = "obs_month"
    , by.y = "obs_date"
    , all = FALSE
)

fullDT[1:5]
```

## 6. Analyzing the data

The purpose of this tutorial was just to teach you how to build a data prep pipeline, so the analysis part will be light. First, let's try using the [corrplot package]() to look at linear correlations between our variables:

```{r}
# Figure out which columns contain data we care about
numeric_cols <- setdiff(names(fullDT), "obs_month")

# Visualize correlations
corrplot::corrplot(
    corr = cor(fullDT[, .SD, .SDcols = numeric_cols])
    , method = "number"
)
```

## 7. Looking at lags

The correlation matrix above is interesting, but it may be misleading. So far, we've looked at raw prices. Raw prices tend to follow long-run trends, and could look highly correlated just because periods of growth intersected. To get a truer picture of how closely these series are related, let's use `data.table`'s `shift` operation to generate month-over-month growth rates instead.

```{r}
# woo! we have 75-ish months of data! Now to make fair comparisons, let's make
# a dataset of % month-over-month growth rates in each of these measures. We can do
# that with the data.table::shift() function
for (colname in setdiff(names(fullDT), "obs_month")){
    new_name <- paste0(colname, "_MoM")
    print(sprintf("Creating %s...", new_name))
    
    fullDT[, (new_name) := (get(colname) - shift(get(colname), n = 1, type = "lag")) / get(colname)]
}

fullDT[1:5]
```

The code below grabs *only* the growth variables (in-line!) and shows their correlations.

```{r}
# Figure out which columns contain data we care about
growth_cols <- grep("MoM", names(fullDT), value = TRUE)

# corrplot() doesn't like NAs
row_idx <- complete.cases(fullDT[, .SD, .SDcols = growth_cols[3:5]])

# Visualize correlations
corrplot::corrplot(
    corr = cor(fullDT[row_idx, .SD, .SDcols = growth_cols[3:5]])
    , method = "number"
)
```

## Conclusion

You did it! Congratulations on making it through this tutorial. You should now have a better understanding of how to build data pipelines in R using `data.table`. If you want to go further or try adding additional enhancements, here is a full version of the code you can paste into a script:

```{r, eval = FALSE, collapse = TRUE}
##################
# Load Libraries #
##################
suppressPackageStartupMessages({
    library(data.table)
    library(Quandl)
    library(quantmod)
    library(lubridate)
    library(corrplot)
})

########################
# Define the functions #
########################

# metals prices
get_metal_prices <- function(qcodes){
    
    responseList <- lapply(
        X = names(qcodes)
        , qcodes = qcodes
        , function(metal_name, qcodes){
            
            print(sprintf("Fetching data for %s...", qcodes[[metal_name]]))
            
            # Grab the dataset and immediately drop columns we don't care about
            responseDT <- data.table::as.data.table(
                Quandl::Quandl(
                    code = qcodes[[metal_name]]
                    , start_date = "2001-12-31"
                    , end_date = "2018-03-12"
                )
            )[, .SD, .SDcols = c("Date", "Cash Buyer")]
            
            # Change names
            data.table::setnames(responseDT, c("obs_date", metal_name))
        }
    )
    
    # merge datasets on time
    outDT <- Reduce(
        f = function(x, y){merge(x, y, all = TRUE, by = "obs_date")}
        , x = responseList
    )
    return(outDT)
}

# macro data
get_macro_data <- function(mcodes){
    responseList <- lapply(
        X = names(mcodes)
        , mcodes = mcodes
        , FUN = function(macro_name, mcodes){
            
            # get data and convert to data.table
            macroDT <- data.table::as.data.table(
                quantmod::getSymbols(
                    Symbols = mcodes[[macro_name]]
                    , src = "FRED"
                    , auto.assign = FALSE
                )
            )
            
            # change names in place
            data.table::setnames(macroDT, c("obs_date", macro_name))
        }
    )
    
    # merge datasets on time
    outDT <- Reduce(
        f = function(x, y){merge(x, y, all = TRUE, by = "obs_date")}
        , x = responseList
    )
    return(outDT)
}

###################
# Get metals data #
###################
spot_codes <- c(
    "cobalt_spot" = "LME/PR_MO"
    , "aluminum_spot" = "LME/PR_AL"
    , "tin_spot" = "LME/PR_TN"
    , "copper_spot" = "LME/PR_CU"
    , "nickel_spot" = "LME/PR_NI"
)

metalsDT <- get_metal_prices(spot_codes)

##################
# Get macro data #
##################
macro_codes <- c(
    "cpi_inflation" = "CPIAUCSL"
    , "pce_inflation" = "PCEPILFE"
)
macroDT <- get_macro_data(macro_codes)

######################################
# Convert metals to monthly averages #
######################################

# define a helper function that takese in a date and produces the corresponding
# date for the 1st day of that date's year and month
.ym <- function(some_date){
    lubridate::make_date(
        year = lubridate::year(some_date)
        , month = lubridate::month(some_date)
        , day = 1L
    )
}

# Use the := operator to add this transformed date to our table
metalsDT[, obs_month := .ym(obs_date)]

# get monthly averages
monthlyMetalsDT <- metalsDT[, lapply(.SD, function(x){mean(x, na.rm = TRUE)})
                           , .SDcols = setdiff(names(metalsDT), c("obs_date", "obs_month"))
                           , by = "obs_month"]

##################################
# join into dataset for analysis #
##################################
fullDT <- merge(
    x = monthlyMetalsDT
    , y = macroDT
    , by.x = "obs_month"
    , by.y = "obs_date"
    , all = FALSE
)

####################################
# Analyze growth rate correlations #
####################################

# woo! we have 75-ish months of data! Now to make fair comparisons, let's make
# a dataset of % month-over-month growth rates in each of these measures. We can do
# that with the data.table::shift() function
for (colname in setdiff(names(fullDT), "obs_month")){
    new_name <- paste0(colname, "_MoM")
    print(sprintf("Creating %s...", new_name))
    
    fullDT[, (new_name) := (get(colname) - shift(get(colname), n = 1, type = "lag")) / get(colname)]
}

# Figure out which columns contain data we care about
growth_cols <- grep("MoM", names(fullDT), value = TRUE)

# corrplot() doesn't like NAs
row_idx <- complete.cases(fullDT[, .SD, .SDcols = growth_cols[3:5]])

# Visualize correlations
corrplot::corrplot(
    corr = cor(fullDT[row_idx, .SD, .SDcols = growth_cols[3:5]])
    , method = "number"
)
```
